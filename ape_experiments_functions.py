import numpy as np
from sklearn.cluster import KMeans
from sklearn.linear_model import LogisticRegression
from yellowbrick.cluster import KElbowVisualizer
from growingspheres import counterfactuals as cf
from sklearn.neighbors import NearestNeighbors
import random
from sklearn.metrics import accuracy_score
from growingspheres.utils.gs_utils import generate_inside_ball, generate_inside_field, \
    generate_categoric_inside_ball, distances
#from anchors.limes.utils_stability import compare_confints

def k_closest_experiments(ape_explainer, k_closest):
    """
    Compute the average distance to the k closest counterfactual from the real dataset to the artificial counterfactual generated by the growing method
    Args: ape_explainer
          k_closest: Number of k closest points to compute the distance to the closest counterfactual
    Return: Average distance to the k closest counterfactual
            Mean of the distance to the whole counterfactual instances from the training set
    """
    index_train_data_counterfactual_class = np.where([x != ape_explainer.target_class for x in ape_explainer.black_box_predict(ape_explainer.train_data)])
    index_train_data_target_class = list(set(range(0, ape_explainer.train_data.shape[0])) - set(index_train_data_counterfactual_class[0]))

    train_data_counterfactual_class, train_data_target_class = ape_explainer.train_data[index_train_data_counterfactual_class], ape_explainer.train_data[index_train_data_target_class]
    avg_dists = []
    
    # Store the mahalanobis distance from either the cluster center (in case of multimodal situation) or the closest counterfactual to 
    # (a) the whole test dataset, (b) test dataset from the counterfactual class, and (c) the test dataset from the target class
    set_set_true_instances = [ape_explainer.test_data, train_data_counterfactual_class, train_data_target_class]
    for set_true_instances in set_set_true_instances:
        if len(set_true_instances) == 0:
            # If there is no instance in the test set from the counterfactual class or the target class
            avg_dists.append(None)
        else:
            try:
                temp_distance = []
                for cluster_center in ape_explainer.clusters_centers:
                    # in case of multimodal situation and clusters centers have been computed
                    temp_distance.append(distances(cluster_center, set_true_instances, ape=ape_explainer, metrics='mahalanobis', dataset=set_true_instances))
                avg_dists.append(abs(np.mean(temp_distance)))
            except Exception:
                # In case of unimodal situation
                avg_dists.append(abs(distances(ape_explainer.closest_counterfactual, set_true_instances, ape=ape_explainer, metrics='mahalanobis', dataset=set_true_instances)))
    print("mahalanobis done", avg_dists)
    
    metrics = ['w_manhattan', 'w_euclidian']
    for metric in metrics:
        # Compute now the distance based on weighted manhattan and weighted euclidean distance
        farthest_distance_cf = 0
        for training_instance in ape_explainer.train_data:
            farthest_distance_cf_now = distances(ape_explainer.closest_counterfactual, training_instance, ape_explainer, metrics=metric)
            if farthest_distance_cf_now > farthest_distance_cf:
                farthest_distance_cf = farthest_distance_cf_now
        ape_explainer.farthest_distance = farthest_distance_cf
        
        neigh = NearestNeighbors(n_neighbors=k_closest+1, algorithm='ball_tree', metric=distances, metric_params={"ape": ape_explainer, "metrics": metric})
        neigh.fit(train_data_counterfactual_class[:1000], ape_explainer.black_box_predict(train_data_counterfactual_class[:1000]))
        try:
            dists, neighs = neigh.kneighbors(ape_explainer.clusters_centers, k_closest+1)
        except AttributeError:
            dists, neighs = neigh.kneighbors(ape_explainer.closest_counterfactual.reshape(1, -1), k_closest+1)
        closest_ennemis = train_data_counterfactual_class[neighs[0]]
        dists = []
        for ennemis in closest_ennemis:
            dists.append(distances(ape_explainer.closest_counterfactual, ennemis, ape_explainer, metrics=metric))
        avg_dists.append(np.mean(dists))
        print(metric, "done")
    
    return avg_dists, None

def compute_all_explanation_method_accuracy(ape_tabular, instance, growing_field, nb_instance_test_data_label_as_target,
                                                position_instances_in_field, instances_in_field, labels_in_field,
                                                farthest_distance_cf, nb_features_employed,
                                                growing_method, linear_model):
    """
    Compute Precision, Coverage and F2 of APE, Anchors, Decision Tree and linear surrogate explanation models
    Args: ape_tabular: Ape tabular object used to explain the target instance
            instance: Target instance to explain
            growing_field: Growing Sphere object used by APE
            nb_instance_test_data_label_as_target: Number of instances from the testing data that are label as the target instance
            position_instances_in_field: Index of the instances from the training data located in the hyper field
            instances_in_field: Set of instances (artificial and training data) located in the hyper field
            labels_in_field: Classification of training instances in the hyper field by the black box model
            farthest_distance: Distance from the target instance to the farthest instance from the training data
            nb_features_employed: Number of features employed by the linear explanation model
            growing_method: The method used to sample artificial instances (between growing spheres and growing fields)
            linear_model: the linear model employed in the linear surrogate (i.e: ridge, linear regression, ...)
    Return: Arrays of accuracy, coverage and F2 of multiple explanation models
    """
    labels_instance_test_data = ape_tabular.black_box_predict(ape_tabular.test_data)
    ape_tabular.instance_to_explain = instance
    # Compute accuracy, coverage and F2 of local surrogate trained over raw data, with an extending field and a logistic regression model as explanation
    local_surrogate_extend_raw_accuracy, local_surrogate_initial_raw_accuracy, local_surrogate_extend_raw_coverage, f2_local_surrogate_extend_raw, _, \
                        ape_tabular.extended_radius = ape_tabular.compute_lime_extending_accuracy_coverage(instances_in_field, 
                                            ape_tabular.closest_counterfactual, labels_in_field, growing_field, nb_features_employed, 
                                            farthest_distance_cf, growing_method, position_instances_in_field, linear_model)
    # Compute accuracy, coverage and F2 for Anchors
    anchor_accuracy, anchor_coverage, f2_anchor, _ = ape_tabular.compute_anchor_accuracy_coverage(instance, 
                                    labels_instance_test_data, len(instances_in_field), 
                                    nb_instance_test_data_label_as_target,
                                    growing_method)
    # Compute accuracy, coverage and F2 for Decision Tree
    decision_tree_accuracy, decision_tree_coverage, f2_decision_tree, _, \
                _ = ape_tabular.compute_decision_tree_accuracy_coverage(instances_in_field, labels_in_field, 
                                        ape_tabular.closest_counterfactual, growing_field.radius, position_instances_in_field)
    # Compute accuracy for classic Local Surrogate
    local_surrogate = ape_tabular.linear_explainer.explain_instance(ape_tabular.closest_counterfactual,
                                                                ape_tabular.black_box_predict_proba,
                                                                model_regressor=linear_model,
                                                                num_features=nb_features_employed)
    # Generate instance for Local Surrogate as in LIME
    local_surrogate_instances, _ = ape_tabular.linear_explainer.data_inverse(ape_tabular.closest_counterfactual,len(instances_in_field))
    # Modify the instances to be classified by the linear surrogate trained on modified instances
    local_surrogate_prediction = ape_tabular.modify_instance_for_linear_model(local_surrogate, local_surrogate_instances)
    local_surrogate_labels = ape_tabular.black_box_predict(local_surrogate_instances)
    accuracy_local_surrogate = {'real':None}
    accuracy_local_surrogate["counterfactual"], accuracy_local_surrogate["auc"]  = \
            ape_tabular.compute_linear_regression_accuracy(local_surrogate_prediction, local_surrogate_labels, local_surrogate.easy_model.intercept_)

    # Compute coverage and F2 for classic Local Surrogate
    position_testing_instances_in_field, nb_testing_instance_in_field = ape_tabular.instances_from_dataset_inside_field(ape_tabular.closest_counterfactual, 
                                                                                                                growing_field.radius, ape_tabular.test_data)
    nb_testing_instance_in_field_label_as_target, _ = ape_tabular.compute_labels_inside_field(nb_testing_instance_in_field, 
                                                                                                                position_testing_instances_in_field,
                                                                                                                ape_tabular.test_data)
    linear_coverage = nb_testing_instance_in_field_label_as_target/nb_instance_test_data_label_as_target
    f2_linear_surrogate = (2*accuracy_local_surrogate['counterfactual'] + linear_coverage)/3
    
    # Compute accuracy for LIME
    lime = ape_tabular.linear_explainer.explain_instance(instance, ape_tabular.black_box_predict_proba,
                                                                model_regressor=linear_model,
                                                                num_features=nb_features_employed)
    # Generate instances
    lime_instances, _ = ape_tabular.linear_explainer.data_inverse(instance,len(instances_in_field))
    # Modify the instances to be classified by the linear surrogate trained on modified instances
    lime_prediction = ape_tabular.modify_instance_for_linear_model(lime, lime_instances)
    lime_labels = ape_tabular.black_box_predict(lime_instances)
    accuracy_lime = {'real':None}
    accuracy_lime["counterfactual"], accuracy_lime["auc"]  = \
            ape_tabular.compute_linear_regression_accuracy(lime_prediction, lime_labels, lime.easy_model.intercept_)

    # Compute coverage and F2 for LIME
    position_testing_instances_in_field, nb_testing_instance_in_field = ape_tabular.instances_from_dataset_inside_field(instance, 
                                                                                                                growing_field.radius, ape_tabular.test_data)
    nb_testing_instance_in_field_label_as_target, _ = ape_tabular.compute_labels_inside_field(nb_testing_instance_in_field, 
                                                                                                                position_testing_instances_in_field,
                                                                                                                ape_tabular.test_data)
    lime_coverage = nb_testing_instance_in_field_label_as_target/nb_instance_test_data_label_as_target
    f2_lime = (2*accuracy_lime['counterfactual'] + linear_coverage)/3

    # Select values for APEa and APEt depending on the unimodality test
    apea_accuracy = anchor_accuracy if ape_tabular.multimodal_results else local_surrogate_extend_raw_accuracy
    apea_coverage = anchor_coverage if ape_tabular.multimodal_results else local_surrogate_extend_raw_coverage
    f2_apea = f2_anchor if ape_tabular.multimodal_results else f2_local_surrogate_extend_raw
    apet_accuracy = decision_tree_accuracy if ape_tabular.multimodal_results else local_surrogate_extend_raw_accuracy
    apet_coverage = decision_tree_coverage if ape_tabular.multimodal_results else local_surrogate_extend_raw_coverage
    f2_apet = f2_decision_tree if ape_tabular.multimodal_results else f2_local_surrogate_extend_raw

    accuracys = [accuracy_local_surrogate["counterfactual"], accuracy_local_surrogate["auc"], local_surrogate_initial_raw_accuracy["counterfactual"], \
                local_surrogate_initial_raw_accuracy["auc"], local_surrogate_extend_raw_accuracy["counterfactual"], \
                local_surrogate_extend_raw_accuracy["auc"], accuracy_lime["counterfactual"], \
                accuracy_lime["auc"], anchor_accuracy['counterfactual'], decision_tree_accuracy['counterfactual'], \
                apea_accuracy['counterfactual'], apet_accuracy['counterfactual']]
    
    coverages = [linear_coverage, linear_coverage, linear_coverage, linear_coverage, local_surrogate_extend_raw_coverage,
                local_surrogate_extend_raw_coverage, lime_coverage, lime_coverage, anchor_coverage, decision_tree_coverage, apea_coverage, apet_coverage]
    
    f2s = [f2_linear_surrogate, f2_linear_surrogate, f2_linear_surrogate, f2_linear_surrogate, \
                f2_local_surrogate_extend_raw, f2_local_surrogate_extend_raw, f2_lime, f2_lime, f2_anchor, f2_decision_tree, f2_apea, f2_apet]
    
    multimodal = 1 if ape_tabular.multimodal_results else 0
    return accuracys, coverages, f2s, multimodal, ape_tabular.extended_radius

def compute_local_surrogate_accuracy_coverage(ape_tabular, instance, growing_field,
                                                test_instances_in_field, test_labels_in_field,
                                                position_testing_instances_in_field, nb_testing_instance_in_field, 
                                                nb_features_employed, growing_method):
    """
    Compute accuracy, coverage and f2 for multiple linear explanation models
    Args: ape_tabular: ape tabular object used to explain the target instance
          instance: target instance to explain
          growing field: growing field object used by APE
          instances_in_field: (artificial and trained) data located in the hyper field
          labels_in_field: Labels by the black box model of the instances located in the hyper field
          position_instances_in_field: Index of the instaces from the training data located in the field
          nb_training_instance_in_field: Number of training data located in the hyper field
          nb_features_employed: Number of features employed by the linear explanations models
    Return: accuracy, coverage and f2 of classic local surrogate, a local surrogate train over training data with a linear regression 
            and a local surrogate using a logistic regression model as explanation
    """
    labels_instance_test_data = ape_tabular.black_box_predict(ape_tabular.test_data)
    nb_instance_test_data_label_as_target = sum(x == ape_tabular.target_class for x in labels_instance_test_data)
    
    nb_testing_instance_in_field_label_as_target, labels_testing_instance_in_field = ape_tabular.compute_labels_inside_field(nb_testing_instance_in_field, 
                                                                                                                        position_testing_instances_in_field,
                                                                                                                        ape_tabular.test_data)    
    
    # Trained a local surrogate model over binarize data using a linear regression (default Local Surrogate)
    local_surrogate = ape_tabular.linear_explainer.explain_instance(ape_tabular.closest_counterfactual, 
                                                                ape_tabular.black_box_predict_proba, 
                                                                num_features=nb_features_employed)
    prediction_inside_field = ape_tabular.modify_instance_for_linear_model(local_surrogate, test_instances_in_field)
    accuracy_local_surrogate, _ = ape_tabular.compute_linear_regression_accuracy(prediction_inside_field,\
        labels_testing_instance_in_field, local_surrogate.easy_model.intercept_)

    # Trained a local surrogate model over training data using a linear regression (by default on Lime)
    local_surogate_raw_data = ape_tabular.linear_explainer.explain_instance_training_dataset(ape_tabular.closest_counterfactual, 
                                                                ape_tabular.black_box_predict_proba, num_features=nb_features_employed,
                                                                instances_in_sphere=test_instances_in_field,
                                                                ape=ape_tabular)
    prediction_inside_field = ape_tabular.modify_instance_for_linear_model(local_surogate_raw_data, test_instances_in_field)
    accuracy_local_surogate_raw_data, _ = ape_tabular.compute_linear_regression_accuracy(prediction_inside_field, \
                labels_testing_instance_in_field, local_surogate_raw_data.easy_model.intercept_)
    
    # Compute accuracy for a local Surrogate model with a Logistic Regression model as explanation model over binary data
    local_surrogate_exp_regression = ape_tabular.linear_explainer.explain_instance(ape_tabular.closest_counterfactual,  
                                                            ape_tabular.black_box_predict, num_features=nb_features_employed,
                                                            model_regressor = LogisticRegression())
    prediction_inside_field = ape_tabular.modify_instance_for_linear_model(local_surrogate_exp_regression, test_instances_in_field)
    accuracy_local_surrogate_logistic_regression = max(accuracy_score(labels_testing_instance_in_field, prediction_inside_field),
                                                        accuracy_score(labels_testing_instance_in_field, prediction_inside_field)) 

    # Compute accuracy, coverage and f2 for Extended Local Surrogate
    accuracy_ls_raw_data, _, lime_extending_coverage, f2_lime_extending, _, _ = ape_tabular.compute_lime_extending_accuracy_coverage(
                                                test_instances_in_field,
                                                ape_tabular.closest_counterfactual, labels_testing_instance_in_field, growing_field, 
                                                nb_features_employed, 1, growing_method, position_testing_instances_in_field)

    if ape_tabular.verbose: print("Computing multiple linear explanation models accuracy and coverage.")
    linear_coverage = nb_testing_instance_in_field_label_as_target/nb_instance_test_data_label_as_target
    f2_lime_regression = (2*accuracy_local_surrogate_logistic_regression+linear_coverage)/3
    f2_not_bin_lime = (2*accuracy_local_surogate_raw_data+linear_coverage)/3
    f2_local_surrogate = (2*accuracy_local_surrogate+linear_coverage)/3
    
    return [accuracy_local_surrogate, accuracy_local_surrogate_logistic_regression, accuracy_local_surogate_raw_data, accuracy_ls_raw_data['counterfactual']], \
                [linear_coverage, linear_coverage, linear_coverage, lime_extending_coverage], \
                    [f2_local_surrogate, f2_lime_regression, f2_not_bin_lime, f2_lime_extending]

def decision_tree_function(clf, instance):
    """
    Args: clf: Trained decision tree model
          instance: Target instance to explain
    Return: the set of features employed by the decision tree model
    """
    feature = clf.tree_.feature
    node_indicator = clf.decision_path(instance)
    leaf_id = clf.apply(instance)

    sample_id = 0
    node_index = node_indicator.indices[node_indicator.indptr[sample_id]:
                                        node_indicator.indptr[sample_id + 1]]
    
    #print('Rules used to predict sample {id}:\n'.format(id=sample_id))
    feature_employed = []
    for node_id in node_index:
        # continue to the next node if it is a leaf node
        if leaf_id[sample_id] == node_id:
            continue
        feature_employed.append(feature[node_id])
    return list(set(feature_employed))

def simulate_user_experiments(ape_tabular, instance, nb_features_employed, farthest_distance, closest_counterfactual, growing_field,
                              position_instances_in_field, nb_training_instance_in_field, only_anchors=False, only_lse=False):
    """
    Function that generate a classic local surrogate explanation, an anchor explanation and an APE explanation and 
    returns the features that are used by these explanation models
    Args: ape_tabular: ape tabular object used to explain the target instance
          instance: target instance to explain
          nb_features_employed: Number of features employed by the linear explanation model
          farthest_distance: Distance from the target instance to explain and the farthest instance from the training data
          closest_counterfactual: Closest instance from the closest counterfactual from the target instance (on the limit of the hyper field 
                                  from the same class as the target instance)
          growing_sphere: growing sphere object used by APE to explain the target instance
          position_instances_in_sphere: Index of the training data located in the hyper field
          nb_training_instance_in_sphere: Number of training data located in the hyper field
    Return: List of features employed by APE, Anchors and a classic local surrogate
    """
    ape_tabular.target_class = ape_tabular.black_box_predict(instance.reshape(1, -1))[0]
    
    # Generates or store instances in the area of the hyperfield and their correspoinding labels
    min_instance_per_class = ape_tabular.nb_min_instance_per_class_in_field
    instances_in_field, _, _ = ape_tabular.generate_instances_inside_field(growing_field.radius, closest_counterfactual, 
                                                                            ape_tabular.train_data, farthest_distance, 
                                                                            min_instance_per_class, position_instances_in_field, 
                                                                            nb_training_instance_in_field, libfolding=False)
    
    # Train a local surrogate extended (learns over the raw data and instances inside the field)
    local_surogate_extended, used_features = ape_tabular.linear_explainer.explain_instance_training_dataset(closest_counterfactual,
                                                                ape_tabular.black_box_predict_proba, 
                                                                num_features=nb_features_employed,
                                                                instances_in_sphere=instances_in_field,
                                                                ape=ape_tabular,
                                                                user_simulated=True)
    features_lse_employed, pos_feat_lse = [], []
    if ape_tabular.verbose: print("LSe explanation", local_surogate_extended.as_list())
    # Store every features employed in the rule that have an impact on the prediction
    for feature_linear_employed in local_surogate_extended.as_list():
        features_lse_employed.append(feature_linear_employed[0])
        if feature_linear_employed[1] > 0:
            pos_feat_lse.append(feature_linear_employed[0])
    _, _, features_employed_in_lse = ape_tabular.generate_rule_and_data_for_anchors(features_lse_employed, 
                                                                                                    ape_tabular.target_class, ape_tabular.train_data, 
                                                                                                    simulated_user_experiment=True)
    features_employed_in_lse = list(set(features_employed_in_lse))
    if len(pos_feat_lse) == 0:
        feat_employed_in_pos_lse = features_employed_in_lse
    else:
        # Store the features employed by local surrogate extended that have a positive impact on the prediction (positive weight of the linear coefficient)
        _, _, feat_employed_in_pos_lse = ape_tabular.generate_rule_and_data_for_anchors(pos_feat_lse, 
                                                                                        ape_tabular.target_class, ape_tabular.train_data, 
                                                                                        simulated_user_experiment=True)
        feat_employed_in_pos_lse = list(set(feat_employed_in_pos_lse))
    features_employed_by_extended_local_surrogate = used_features

    if only_lse: return features_employed_by_extended_local_surrogate
    
    # Compute an anchor explanation to compare with APE and local Surrogate
    anchor_exp = ape_tabular.anchor_explainer.explain_instance(instance, ape_tabular.black_box_predict, threshold=ape_tabular.threshold_precision, 
                                delta=0.1, tau=0.15, batch_size=100, max_anchor_size=None, stop_on_first=False,
                                desired_label=None, beam_size=4)
    if ape_tabular.verbose: print("rule by anchor", anchor_exp.names())
    # Transform the explanation generated by Anchors to know what are the features employed by Anchors
    _, _, features_employed_in_rule = ape_tabular.generate_rule_and_data_for_anchors(anchor_exp.names(), 
                                                                                            ape_tabular.target_class, ape_tabular.train_data, 
                                                                                            simulated_user_experiment=True)
    features_employed_in_rule = list(set(features_employed_in_rule))
    if only_anchors:
        return features_employed_in_rule

    if not ape_tabular.multimodal_results:
        # In case of unimodal data we compute a local surrogate explanation trained over raw instances located in the hyper sphere 
        local_surogate = ape_tabular.linear_explainer.explain_instance_training_dataset(closest_counterfactual,
                                                                    ape_tabular.black_box_predict_proba, 
                                                                    num_features=nb_features_employed,
                                                                    instances_in_sphere=instances_in_field,
                                                                    ape=ape_tabular)
        features_linear_employed = []
        for feature_linear_employed in local_surogate.as_list():
            features_linear_employed.append(feature_linear_employed[0])

        # Transform the explanation generated by Local Surrogate to know what are the features employed by LS
        _, _, features_employed_by_extended_local_surrogate = ape_tabular.generate_rule_and_data_for_anchors(features_linear_employed, 
                                                                                                    ape_tabular.target_class, ape_tabular.train_data, 
                                                                                                    simulated_user_experiment=True)

        features_employed_by_extended_local_surrogate = list(set(features_employed_by_extended_local_surrogate))
        features_employed_by_ape = features_employed_by_extended_local_surrogate
    else:
        # In case of multimodal data APE choose an anchor explanation 
        features_employed_by_ape = features_employed_in_rule
    
    # Generate a classic local Surrogate explanation model to compare with APE and Anchors
    local_surrogate = ape_tabular.linear_explainer.explain_instance(closest_counterfactual,
                                                                ape_tabular.black_box_predict_proba, 
                                                                num_features=nb_features_employed)#len(features_employed_in_rule))
    features_linear_employed, pos_feat_ls = [], []
    if ape_tabular.verbose: print("classic local surogate", local_surrogate.as_list())
    # Store features employed in classic local surrogate
    for feature_linear_employed in local_surrogate.as_list():
        features_linear_employed.append(feature_linear_employed[0])
        if feature_linear_employed[1] > 0:
            pos_feat_ls.append(feature_linear_employed[0])
    _, _, features_employed_in_linear = ape_tabular.generate_rule_and_data_for_anchors(features_linear_employed, 
                                                                                                    ape_tabular.target_class, ape_tabular.train_data, 
                                                                                                    simulated_user_experiment=True)
    features_employed_in_linear = list(set(features_employed_in_linear))
    if len(pos_feat_ls) == 0:
        feat_employed_in_pos_ls = features_employed_in_linear
    else:
        _, _, feat_employed_in_pos_ls = ape_tabular.generate_rule_and_data_for_anchors(pos_feat_ls, 
                                                                                        ape_tabular.target_class, ape_tabular.train_data, 
                                                                                        simulated_user_experiment=True)
        feat_employed_in_pos_ls = list(set(feat_employed_in_pos_ls))

    # Generate a classic local Surrogate explanation model to compare with APE and Anchors
    lime = ape_tabular.linear_explainer.explain_instance(instance,
                                                                ape_tabular.black_box_predict_proba, 
                                                                num_features=nb_features_employed)#len(features_employed_in_rule))
    features_lime_employed, pos_feat_lime = [], []
    if ape_tabular.verbose: print("LIME", local_surrogate.as_list())
    # Store features employed in LIME
    for feature_linear_employed in lime.as_list():
        features_lime_employed.append(feature_linear_employed[0])
        if feature_linear_employed[1] > 0:
            pos_feat_lime.append(feature_linear_employed[0])
    _, _, features_employed_in_lime = ape_tabular.generate_rule_and_data_for_anchors(features_lime_employed, 
                                                                                                    ape_tabular.target_class, ape_tabular.train_data, 
                                                                                                    simulated_user_experiment=True)
    features_employed_in_lime = list(set(features_employed_in_lime))
    if len(pos_feat_lime) == 0:
        features_employed_in_pos_lime = features_employed_in_lime
    else:
        _, _, features_employed_in_pos_lime = ape_tabular.generate_rule_and_data_for_anchors(pos_feat_lime, 
                                                                                                    ape_tabular.target_class, ape_tabular.train_data, 
                                                                                                    simulated_user_experiment=True)
        features_employed_in_pos_lime = list(set(features_employed_in_pos_lime))

    return features_employed_in_linear, feat_employed_in_pos_ls, features_employed_in_lime, features_employed_in_pos_lime, \
        features_employed_by_extended_local_surrogate, feat_employed_in_pos_lse, features_employed_in_lse, \
        features_employed_by_ape, features_employed_in_rule, \
        ape_tabular.multimodal_results, growing_field.radius